{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a23d973-6da4-4f68-a505-7d6aa3704416",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Generowanie danych: normal + outliers\n",
    "# -----------------------------\n",
    "\n",
    "n_total = 300_000\n",
    "frac_outliers = 0.02\n",
    "n_out = int(n_total * frac_outliers)\n",
    "n_norm = n_total - n_out\n",
    "\n",
    "# Dane normalne ~ N(0,1)\n",
    "df_norm = (spark.range(n_norm).select(F.col(\"id\").alias(\"row_id\"))\n",
    ".withColumn(\"x1\", F.randn(1))\n",
    ".withColumn(\"x2\", F.randn(2))\n",
    ".withColumn(\"x3\", F.randn(3))\n",
    ".withColumn(\"is_outlier\", F.lit(0))\n",
    ")\n",
    "\n",
    "# Outliery ~ N(8,1.5)\n",
    "df_out = (spark.range(n_out).select((F.col(\"id\") + n_norm).alias(\"row_id\"))\n",
    "          .withColumn(\"x1\", 8.0 + 1.5 * F.randn(11))\n",
    "            .withColumn(\"x2\", 8.0 + 1.5 * F.randn(12))\n",
    "            .withColumn(\"x3\", 8.0 + 1.5 * F.randn(13))\n",
    "            .withColumn(\"is_outlier\", F.lit(1))\n",
    ")\n",
    "\n",
    "df = df_norm.unionByName(df_out).orderBy(F.rand(999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9111ff9c-6026-48c3-942f-b548c5eaee18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2) Kontrakt cech: vector + standardization (Spark)\n",
    "# -----------------------------\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"x1\", \"x2\", \"x3\"], outputCol=\"features_raw\")\n",
    "df_vec = assembler.transform(df)\n",
    "scaler = StandardScaler(inputCol=\n",
    "\"features_raw\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "scaler_model = scaler.fit(df_vec)\n",
    "df_scaled = scaler_model.transform(df_vec).select(\"row_id\", \"features\", \"is_outlier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1285f744-c90a-428f-9d50-54fde9f51231",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3) Autoenkoder: trening na NORMAL (sample) w TensorFlow\n",
    "# -----------------------------\n",
    "\n",
    "train_pd = (df_scaled\n",
    ".filter(F.col(\"is_outlier\") == 0)\n",
    ".sample(withReplacement=False, fraction=0.10, seed=42)\n",
    ".select(\"features\")\n",
    ".toPandas()\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.vstack(train_pd[\"features\"].values).astype(np.float32)\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "inputs = tf.keras.Input(shape=(input_dim,))\n",
    "h = tf.keras.layers.Dense(8, activation=\"relu\")(inputs)\n",
    "z = tf.keras.layers.Dense(2, activation=\"relu\")(h) h2 = tf.keras.layers.Dense(8, activation=\"relu\")(z)\n",
    "outputs = tf.keras.layers.Dense(input_dim)(h2)\n",
    "\n",
    "# latent\n",
    "\n",
    "autoencoder = tf.keras.Model(inputs, outputs)\n",
    "autoencoder.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "autoencoder.fit(X_train, X_train, epochs=8, batch_size=256, verbose=0)\n",
    "MODEL_PATH = \"/tmp/ae_model.keras\"\n",
    "autoencoder.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d01df08e-320c-45f4-9931-fc317b9e601a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4) Skoring w Spark: reconstruction error jako anomaly_score (mapInPandas)\n",
    "# -----------------------------\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, LongType, DoubleType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"row_id\", LongType(), False),\n",
    "    StructField(\"anomaly_score\", DoubleType(), False),\n",
    "    StructField(\"is_outlier\", IntegerType(), False),\n",
    "])\n",
    "\n",
    "def score_with_autoencoder(pdf_iter):\n",
    "\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "\n",
    "    model = tf.keras.models.load_model(MODEL_PATH)\n",
    "\n",
    "    for pdf in pdf_iter:\n",
    "        X = np.vstack(pdf[\"features\"].values).astype(np.float32)\n",
    "        recon = model.predict(X, verbose=0)\n",
    "        err = np.mean((X - recon) ** 2, axis=1) # MSE per-row\n",
    "        out = pd.DataFrame({\n",
    "            \"row_id\": pdf[\"row_id\"].values.astype(np.int64),\n",
    "            \"anomaly_score\": err.astype(np.float64),\n",
    "            \"is_outlier\": pdf[\"is_outlier\"].values.astype(np.int32),\n",
    "        })\n",
    "        yield out\n",
    "        \n",
    "scored = df_scaled.mapInPandas(score_with_autoencoder, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5d2d308-2d07-4b39-b074-44ecee54beec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5) PrÂ´og i flaga anomalii (percentyl)\n",
    "# -----------------------------\n",
    "threshold = scored.approxQuantile(\"anomaly_score\", [0.995], 0.01)[0]\n",
    "scored = scored.withColumn(\"is_anomaly\", (F.col(\"anomaly_score\") >= F.lit(threshold)).cast(\"int\"))\n",
    "print(\"Threshold (approx 99.5% quantile):\", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a1c6247-f74c-4179-9212-4c5eb6130017",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 6) Ewaluacja (etykieta is_outlier tylko do oceny)\n",
    "# -----------------------------\n",
    "cm = (scored.groupBy(\"is_outlier\", \"is_anomaly\").count().orderBy(\"is_outlier\", \"is_anomaly\"))\n",
    "cm.show(truncate=False)\n",
    "tp = scored.filter(\"is_outlier=1 AND is_anomaly=1\").count()\n",
    "fp = scored.filter(\"is_outlier=0 AND is_anomaly=1\").count()\n",
    "fn = scored.filter(\"is_outlier=1 AND is_anomaly=0\").count()\n",
    "precision = tp / (tp + fp) if (tp + fp) else 0.0\n",
    "recall = tp / (tp + fn) if (tp + fn) else 0.0\n",
    "print(f\"TP={tp} FP={fp} FN={fn}\")\n",
    "print(f\"Precision={precision:.4f} Recall={recall:.4f}\")\n",
    "scored.orderBy(F.desc(\"anomaly_score\")).show(20, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Kod z PDF",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}