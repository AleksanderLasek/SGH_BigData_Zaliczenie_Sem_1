{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eca89d0-e070-49dc-9541-772d6ad9cd30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (4.5 kB)\nCollecting absl-py>=1.0.0 (from tensorflow)\n  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\nRequirement already satisfied: astunparse>=1.6.0 in /databricks/python3/lib/python3.12/site-packages (from tensorflow) (1.6.3)\nCollecting flatbuffers>=24.3.25 (from tensorflow)\n  Downloading flatbuffers-25.12.19-py2.py3-none-any.whl.metadata (1.0 kB)\nCollecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n  Downloading gast-0.7.0-py3-none-any.whl.metadata (1.5 kB)\nCollecting google_pasta>=0.1.1 (from tensorflow)\n  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\nCollecting libclang>=13.0.0 (from tensorflow)\n  Downloading libclang-18.1.1-py2.py3-none-manylinux2014_aarch64.whl.metadata (5.2 kB)\nCollecting opt_einsum>=2.3.2 (from tensorflow)\n  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.12/site-packages (from tensorflow) (24.1)\nRequirement already satisfied: protobuf>=5.28.0 in /databricks/python3/lib/python3.12/site-packages (from tensorflow) (5.29.4)\nRequirement already satisfied: requests<3,>=2.21.0 in /databricks/python3/lib/python3.12/site-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (74.0.0)\nRequirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\nCollecting termcolor>=1.1.0 (from tensorflow)\n  Downloading termcolor-3.3.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: typing_extensions>=3.6.6 in /databricks/python3/lib/python3.12/site-packages (from tensorflow) (4.12.2)\nRequirement already satisfied: wrapt>=1.11.0 in /databricks/python3/lib/python3.12/site-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /databricks/python3/lib/python3.12/site-packages (from tensorflow) (1.67.0)\nCollecting tensorboard~=2.20.0 (from tensorflow)\n  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\nCollecting keras>=3.10.0 (from tensorflow)\n  Downloading keras-3.13.0-py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: numpy>=1.26.0 in /databricks/python3/lib/python3.12/site-packages (from tensorflow) (2.1.3)\nCollecting h5py>=3.11.0 (from tensorflow)\n  Downloading h5py-3.15.1-cp312-cp312-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl.metadata (3.0 kB)\nCollecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n  Downloading ml_dtypes-0.5.4-cp312-cp312-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl.metadata (8.9 kB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\nRequirement already satisfied: rich in /databricks/python3/lib/python3.12/site-packages (from keras>=3.10.0->tensorflow) (13.9.4)\nCollecting namex (from keras>=3.10.0->tensorflow)\n  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\nCollecting optree (from keras>=3.10.0->tensorflow)\n  Downloading optree-0.18.0-cp312-cp312-manylinux_2_26_aarch64.manylinux_2_28_aarch64.whl.metadata (34 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\nCollecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n  Downloading markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\nRequirement already satisfied: pillow in /databricks/python3/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (11.1.0)\nCollecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\nCollecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n  Downloading werkzeug-3.1.5-py3-none-any.whl.metadata (4.0 kB)\nRequirement already satisfied: markupsafe>=2.1.1 in /databricks/python3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /databricks/python3/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow) (2.2.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /databricks/python3/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow) (2.15.1)\nRequirement already satisfied: mdurl~=0.1 in /databricks/python3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.0)\nDownloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (259.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/259.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[91m━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m38.3/259.5 MB\u001B[0m \u001B[31m208.3 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m89.1/259.5 MB\u001B[0m \u001B[31m231.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m137.6/259.5 MB\u001B[0m \u001B[31m234.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━\u001B[0m \u001B[32m188.2/259.5 MB\u001B[0m \u001B[31m238.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━\u001B[0m \u001B[32m233.8/259.5 MB\u001B[0m \u001B[31m236.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m259.5/259.5 MB\u001B[0m \u001B[31m235.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m259.5/259.5 MB\u001B[0m \u001B[31m235.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m259.5/259.5 MB\u001B[0m \u001B[31m235.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m259.5/259.5 MB\u001B[0m \u001B[31m235.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m259.5/259.5 MB\u001B[0m \u001B[31m235.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m259.5/259.5 MB\u001B[0m \u001B[31m126.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\nDownloading flatbuffers-25.12.19-py2.py3-none-any.whl (26 kB)\nDownloading gast-0.7.0-py3-none-any.whl (22 kB)\nDownloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\nDownloading h5py-3.15.1-cp312-cp312-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl (4.9 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/4.9 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.9/4.9 MB\u001B[0m \u001B[31m149.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading keras-3.13.0-py3-none-any.whl (1.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.5/1.5 MB\u001B[0m \u001B[31m96.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2014_aarch64.whl (23.8 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/23.8 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m23.8/23.8 MB\u001B[0m \u001B[31m196.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading ml_dtypes-0.5.4-cp312-cp312-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl (5.0 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/5.0 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.0/5.0 MB\u001B[0m \u001B[31m145.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\nDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/5.5 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.5/5.5 MB\u001B[0m \u001B[31m117.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading termcolor-3.3.0-py3-none-any.whl (7.7 kB)\nDownloading markdown-3.10-py3-none-any.whl (107 kB)\nDownloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\nDownloading werkzeug-3.1.5-py3-none-any.whl (225 kB)\nDownloading namex-0.1.0-py3-none-any.whl (5.9 kB)\nDownloading optree-0.18.0-cp312-cp312-manylinux_2_26_aarch64.manylinux_2_28_aarch64.whl (364 kB)\nInstalling collected packages: namex, libclang, flatbuffers, werkzeug, termcolor, tensorboard-data-server, optree, opt_einsum, ml_dtypes, markdown, h5py, google_pasta, gast, absl-py, tensorboard, keras, tensorflow\nSuccessfully installed absl-py-2.3.1 flatbuffers-25.12.19 gast-0.7.0 google_pasta-0.2.0 h5py-3.15.1 keras-3.13.0 libclang-18.1.1 markdown-3.10 ml_dtypes-0.5.4 namex-0.1.0 opt_einsum-3.4.0 optree-0.18.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.3.0 werkzeug-3.1.5\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d79e93e8-d6bc-4a67-b47e-a112dff33e75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 57.640869140625\n+----------+----------+------+\n|is_outlier|is_anomaly| count|\n+----------+----------+------+\n|         0|         0|294000|\n|         1|         0|  5999|\n|         1|         1|     1|\n+----------+----------+------+\n\nTP=1, FP=0, FN=5999\nPrecision=1.0000, Recall=0.0002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, LongType, DoubleType, IntegerType\n",
    "\n",
    "# Inicjalizacja Sparka\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# ==========================================\n",
    "# 1. Generowanie danych (Zadanie A)\n",
    "# ==========================================\n",
    "n_total = 300_000\n",
    "frac_outliers = 0.02\n",
    "n_out = int(n_total * frac_outliers)\n",
    "n_norm = n_total - n_out\n",
    "\n",
    "# Dane normalne ~ N(0,1)\n",
    "df_norm = (spark.range(n_norm).select(F.col(\"id\").alias(\"row_id\"))\n",
    "           .withColumn(\"x1\", F.randn(1))\n",
    "           .withColumn(\"x2\", F.randn(2))\n",
    "           .withColumn(\"x3\", F.randn(3))\n",
    "           .withColumn(\"is_outlier\", F.lit(0)))\n",
    "\n",
    "# Outliery ~ N(8, 1.5)\n",
    "df_out = (spark.range(n_out).select((F.col(\"id\") + n_norm).alias(\"row_id\"))\n",
    "          .withColumn(\"x1\", 8.0 + 1.5 * F.randn(11))\n",
    "          .withColumn(\"x2\", 8.0 + 1.5 * F.randn(12))\n",
    "          .withColumn(\"x3\", 8.0 + 1.5 * F.randn(13))\n",
    "          .withColumn(\"is_outlier\", F.lit(1)))\n",
    "\n",
    "df = df_norm.unionByName(df_out).orderBy(F.rand(seed=42))\n",
    "\n",
    "# ==========================================\n",
    "# 2. Kontrakt cech (RĘCZNA STANDARYZACJA)\n",
    "# ==========================================\n",
    "# Omijamy pyspark.ml.StandardScaler, który wywala błędy na Serverless.\n",
    "# Liczymy statystyki globalne:\n",
    "stats = df.select(\n",
    "    F.mean(\"x1\").alias(\"m1\"), F.stddev(\"x1\").alias(\"s1\"),\n",
    "    F.mean(\"x2\").alias(\"m2\"), F.stddev(\"x2\").alias(\"s2\"),\n",
    "    F.mean(\"x3\").alias(\"m3\"), F.stddev(\"x3\").alias(\"s3\")\n",
    ").collect()[0]\n",
    "\n",
    "# Wyciągamy wartości do zmiennych\n",
    "m1, s1 = stats[\"m1\"], stats[\"s1\"]\n",
    "m2, s2 = stats[\"m2\"], stats[\"s2\"]\n",
    "m3, s3 = stats[\"m3\"], stats[\"s3\"]\n",
    "\n",
    "# Aplikujemy wzór: (x - mean) / std\n",
    "df_scaled = df.withColumn(\"x1_s\", (F.col(\"x1\") - m1) / s1) \\\n",
    "              .withColumn(\"x2_s\", (F.col(\"x2\") - m2) / s2) \\\n",
    "              .withColumn(\"x3_s\", (F.col(\"x3\") - m3) / s3) \\\n",
    "              .select(\"row_id\", \"x1_s\", \"x2_s\", \"x3_s\", \"is_outlier\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. Autoenkoder: Trening na próbce (Driver)\n",
    "# ==========================================\n",
    "train_pd = (df_scaled\n",
    "            .filter(F.col(\"is_outlier\") == 0)\n",
    "            .sample(withReplacement=False, fraction=0.10, seed=42)\n",
    "            .select(\"x1_s\", \"x2_s\", \"x3_s\") # Bierzemy 3 kolumny zamiast wektora\n",
    "            .toPandas())\n",
    "\n",
    "# Ręczne sklejenie kolumn do macierzy numpy\n",
    "X_train = train_pd[[\"x1_s\", \"x2_s\", \"x3_s\"]].values.astype(np.float32)\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "# Funkcja budująca model\n",
    "def build_autoencoder(input_dim):\n",
    "    inputs = tf.keras.Input(shape=(input_dim,))\n",
    "    h = tf.keras.layers.Dense(8, activation=\"relu\")(inputs)\n",
    "    z = tf.keras.layers.Dense(2, activation=\"relu\")(h)\n",
    "    h2 = tf.keras.layers.Dense(8, activation=\"relu\")(z)\n",
    "    outputs = tf.keras.layers.Dense(input_dim)(h2)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "# Trening\n",
    "autoencoder = build_autoencoder(input_dim)\n",
    "autoencoder.fit(X_train, X_train, epochs=8, batch_size=256, verbose=0)\n",
    "\n",
    "# Wagi do zmiennej globalnej (mechanizm Closure dla workerów)\n",
    "GLOBAL_MODEL_WEIGHTS = autoencoder.get_weights()\n",
    "\n",
    "# ==========================================\n",
    "# 4. Skoring w Spark (mapInPandas)\n",
    "# ==========================================\n",
    "schema = StructType([\n",
    "    StructField(\"row_id\", LongType(), False),\n",
    "    StructField(\"anomaly_score\", DoubleType(), False),\n",
    "    StructField(\"is_outlier\", IntegerType(), False),\n",
    "])\n",
    "\n",
    "def score_with_autoencoder(pdf_iter):\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    # Odtwarzamy model na workerze\n",
    "    local_model = build_autoencoder(input_dim=3)\n",
    "    local_model.set_weights(GLOBAL_MODEL_WEIGHTS)\n",
    "    \n",
    "    for pdf in pdf_iter:\n",
    "        # Zamiast wyciągać z wektora, sklejamy 3 kolumny\n",
    "        X = pdf[[\"x1_s\", \"x2_s\", \"x3_s\"]].values.astype(np.float32)\n",
    "        \n",
    "        recon = local_model.predict(X, verbose=0)\n",
    "        err = np.mean((X - recon) ** 2, axis=1)\n",
    "        \n",
    "        yield pd.DataFrame({\n",
    "            \"row_id\": pdf[\"row_id\"].values.astype(np.int64),\n",
    "            \"anomaly_score\": err.astype(np.float64),\n",
    "            \"is_outlier\": pdf[\"is_outlier\"].values.astype(np.int32),\n",
    "        })\n",
    "\n",
    "scored = df_scaled.mapInPandas(score_with_autoencoder, schema=schema)\n",
    "\n",
    "# ==========================================\n",
    "# 5. Próg i Ewaluacja\n",
    "# ==========================================\n",
    "# scored.cache()  <-- Wyłączone dla Serverless\n",
    "\n",
    "threshold = scored.approxQuantile(\"anomaly_score\", [0.995], 0.01)[0]\n",
    "print(f\"Threshold: {threshold}\")\n",
    "\n",
    "scored = scored.withColumn(\"is_anomaly\", (F.col(\"anomaly_score\") >= F.lit(threshold)).cast(\"int\"))\n",
    "\n",
    "# Macierz pomyłek\n",
    "cm = scored.groupBy(\"is_outlier\", \"is_anomaly\").count().orderBy(\"is_outlier\", \"is_anomaly\")\n",
    "cm.show()\n",
    "\n",
    "# Metryki\n",
    "tp = scored.filter(\"is_outlier=1 AND is_anomaly=1\").count()\n",
    "fp = scored.filter(\"is_outlier=0 AND is_anomaly=1\").count()\n",
    "fn = scored.filter(\"is_outlier=1 AND is_anomaly=0\").count()\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "\n",
    "print(f\"TP={tp}, FP={fp}, FN={fn}\")\n",
    "print(f\"Precision={precision:.4f}, Recall={recall:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Kod z PDF - _Poprawiony",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}